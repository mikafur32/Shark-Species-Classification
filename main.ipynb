{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "from torch import optim\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from Build_Histogram import *\n",
    "from Detect_Feature_And_KeyPoints import *\n",
    "from Load_Dataset_Folder import *\n",
    "from Features_Processing import *\n",
    "from Linear_Processsing_Pipeline import *\n",
    "from Training_Poly_Processing_Pipeline import *\n",
    "from Testing_Poly_Processing_Pipeline import *\n",
    "from calculate_accuracy import *\n",
    "from train import *\n",
    "from evaluate import *\n",
    "from LeNet_Implementation import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.utils.data as data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Custom CNN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "user= 'perso'\n",
    "root_path = \"C:\\\\Users\\\\\" + user + \"\\\\Documents\\\\GitHub\\\\Shark-Species-Classification\"\n",
    "data_path = os.path.join(root_path, 'Genus Carcharhinus')\n",
    "\n",
    "\n",
    "dataset = ImageLoader(data_path)\n",
    "\n",
    "train_dataset, test_dataset = Dataset_Splitter(.5, dataset)\n",
    "train_train_dataset, validation_dataset = Dataset_Splitter(.9, train_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_iterator = data.DataLoader(train_train_dataset,\n",
    "                                 shuffle=True,\n",
    "                                 batch_size=BATCH_SIZE)\n",
    "\n",
    "valid_iterator = data.DataLoader(validation_dataset,\n",
    "                                 batch_size=BATCH_SIZE)\n",
    "\n",
    "test_iterator = data.DataLoader(test_dataset,\n",
    "                                batch_size=BATCH_SIZE)\n",
    "\n",
    "OUTPUT_DIM = 9\n",
    "model = MMNet(OUTPUT_DIM)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "device = torch.device('cuda') #if torch.cuda.is_available() else 'cpu')\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "EPOCHS = [1,3,5,10,15,20,30]\n",
    "EPOCHS = 7\n",
    "best_epoch = 0\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "'''\n",
    "\n",
    "'''\n",
    "for epoch_ in trange(EPOCHS, desc=\"Epochs\"):\n",
    "\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion, device)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion, device)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'MMnet-model.pt')\n",
    "        best_epoch = epoch_\n",
    "\n",
    "    print(f'Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## VGG Pretrained"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SVC & KMeans"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "image_files, labels = load_dataset_folder(data_path)\n",
    "features, processed_labels = Features_Processing(image_files, labels)\n",
    "\n",
    "\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, processed_labels, test_size = .2, random_state = 0)\n",
    "features_train_train, features_validation, labels_train_train, labels_validation = train_test_split(features_train, labels_train, test_size = .25, random_state = 0)\n",
    "\n",
    "\n",
    "c, d = Poly_Processing_Pipeline(features_train_train, features_validation, labels_train_train, labels_validation)\n",
    "\n",
    "\n",
    "training_accuracy = Testing_Poly_SVC(features_train_train, features_validation, labels_train_train, labels_validation, c, d)\n",
    "testing_accuracy = Testing_Poly_SVC(features_train, features_test, labels_train, labels_test,c, d)\n",
    "\n",
    "print(\"c: \", c)\n",
    "print(\"d: \", d)\n",
    "print(\"Training Accuracy: \", training_accuracy)\n",
    "print(\"Testing Accuracy: \", testing_accuracy)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}